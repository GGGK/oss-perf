Generally, several dimensions are compared on each graph so that 
(ideally) a **conclusion for action** can be reached based on what is shown.

<a id="ScopeDimensions">
## Scope Dimensions</a>
Among the hierarchies referenced to "slice and dice" performance observations:

 a. time (peak)

 b. approach to build server under test

 c. server configuration set

 d. release/version of app

 e. project

 f. sub-systems

 g. sequence flow of functionality being tested

 h. tester

 i. keywords, etc.


<a id="MetricDimensions">
## Metric Dimensions</a>
Among the dozens of what is measured are these most referenced:

* Response time (speed).
* Average time in queue

* Rate of processing (hits per second, business transactions per hour, etc.).
* Megabytes/Gigabytes transferred per second.
* Count of files transferred.

* Queue length. This is transient.
* Threads, Processed used.

* Megabytes/Gigabytes transferred per second.
* Memory used and memory free.
* CPU percent used.
* Megahertz used (in VM).

* Garbage collection events (major or minor).

* etc.

<a id="StatisticalDimensions">
## Statistical Dimensions</a>
For each of the above:

  * Average (Mean)
  * Median
  * 90th percentile.
  * Standard deviation
  * Coefficient of variation

<a id="ComparativeDimensions">
## Comparative Dimensions</a>
Examples:

 * Timing of garbage collection vs. response time.
